
.. Optimization blog post example, created by `ablog start` on Jul 29, 2018.

.. post:: Jul 29, 2018
   :tags: atag
   :author: Pashmina Cameron

Python
==========

Discussion on Cholesky decomposition in Python

*************
LLVM Numba
*************

LLVM provides a way to debug the vectorization performance of Numba. 

LLVM outputs the generated assembly for each line of code - Great if you know assembly. If not, there are some useful pointers to understand what's going on without having to learn assembly (I recommend learning assembly!)::

    add LLVM output here

From LLVM lib/Analysis/CostAnalysis.cpp inline documentation,::

    The cost results are unit-less and the cost number represents the 
    throughput of the machine assuming that all loads hit the cache, 
    all branches are predicted, etc.

LLVM logs may show something along the lines of::

    LV: Found an estimated cost of 1 for VF 4 For instruction: 
    %.523.i = add i64 %.522.i, %.47.fca.4.load3

You may see, for example, statements like:: 

    LV: We can vectorize this loop!

or ::

    LV: Not vectorizing: loop did not meet vectorization requirements.

Having all loops vectorized may not lead to fastest code. For example, using fastmath=True forced LLVM to vectorize a loop, but led to slower code. 

Also, see @vectorize (useful for creating vectorized functions that perform pairwise operations on the elements of an array) and @guvectorize (same as @vectorize, but allows operation over a subset of elements of an array) decorators for creating ufuncs. Note that, you cannot mix @vectorize, @guvectorize with @numba.jit vectorization. You need to choose the type of vectorization that suits your code the best and stick with it. 

@numba.jitclass decorator allows entire classes to be JIT-compiled. A type specification of the members of the class needs to be provided; this allows the member data to be stored in C format, allowing member functions access to the raw data without having to go through the interpreter. Great - more like C. This is great, if you understand C/C++. If you don't understand C/C++, it can be hard to debug mistakes, especially as the interpreter support is limited in this mode, but it can be a boon for those who know C/C++ and want to bypass the interpreter. Recommended in late stages of optimization when you have run out of all options and `still` want to stick with Python in order to avoid retraining your users.

Also, see @cfunc decorator that allows you to create C-compatible callbacks for Python functions. 

Analyzing performance of Cholesky decomposition

We use the A = L L* variant of Cholesky decomposition here. Using A = L D L* variant has some advantages (avoids having to compute square roots), which we will discuss later, but let's start with the simple case first. We chose Cholesky decomposition because it is used in a wide variety of fields, is well understood, there are highly optimized versions of the algorithms available in every language for us to benchmark performance. Reasonably efficient implementations of Cholesky decomposition have been around for a while now, but `efficient` is a moving target in the face of ever-changing hardware baseline. This remains an active area of research [#f2]_.

We will review the following variants of the same algorithm

1. Python implementation
2. NumPy implementation (uses Numpy matrices but does not use np.linalg package)
3. Numba implementation
4. numpy.linalg.cholesky 
5. scipy.linalg.cholesky
6. scipy.linalg.lapack.cholesky

We use numpy and scipy implementations to set ourselves a target, rather than to evaluate them. The aim here is to see how far we can go from a pure Python implementation to Numba one without putting in a lot of effort or sacrificing readbility of the code [#f1]_ .  

Note on timings: 

First, try to get a good idea of where the time is being spent by using a line profiler. A good utility for getting line by line results is line_profiler. Use ``pip install line_profiler`` to get it set up. Add an ``@profile`` decorator to the functions you want to profile. Then use ``kernprof -l -v script.py`` to see results of running profiling on ``script.py``. The output will be something like this

.. code-block:: guess

    Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
        20                                           @profile
        21                                           def cholesky_py_enum(A, L):
        22       101        359.0      3.6      0.1      for i, (Ai, Li) in enumerate(zip(A, L)):
        23      5150      11465.0      2.2      4.1          for j, Lj in enumerate(L[:i+1]):
        24      5050     238963.0     47.3     86.5              s = sum(Li[k] * Lj[k] for k in range(j))
        25      5050      11311.0      2.2      4.1              Li[j] = sqrt(Ai[i] - s) if (i == j) else \
        26      4950      14249.0      2.9      5.2                      (1.0 / Lj[j] * (Ai[j] - s))
        27         1          2.0      2.0      0.0      return

Takeaways: 

* If there is an optimized implementation available for your algorithm (such as scipy.linalg.lapack.cholesky), use that instead! 
* Numba can get you pretty close to optimized algorithms with very little effort (if the algorithm lends itself to vectorization). This is useful when you want to optimize *your* code, not a standard linear algebra routine. 

.. rubric:: Footnotes

.. [#f2] Cholesky Factorization on SIMD multi-core architectures `Paper <https://hal.archives-ouvertes.fr/hal-01550129/document>`_
.. [#f1] If we have to sacrifice readability to such an extent that a new user cannot immediately comprehend the code, then we might as well write the code in a lower level language such as C/C++ and maximise speed gains.


