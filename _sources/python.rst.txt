
.. Optimization blog post created on Jul 29, 2018.

.. post:: Jul 29, 2018
   :tags: atag
   :author: Pashmina Cameron

Optimizing Python
====================

Benchmarks were taken on an Intel Xeon E5 processor (Windows 10). Intallation instructions refer to Windows environment as well, but they are pretty similar for Linux. This has been tested on Linux and WSL as well. 

************
Profiling
************

In order to get a good understanding of the performance bottlenecks, one can use any of Python's standard profiling tools, but most of them only time function calls. If the slow functions are large, then it is helpful to to get a good idea of which lines the time is being spent in. A good utility for getting line by line timing is :code:`line_profiler`. Use :code:`pip install line_profiler` to install it. Add an :code:`@profile` decorator to the functions you want to profile. Then use :code:`kernprof -l -v script.py` to see results of running profiling on :code:`script.py`. The output will look like this:

.. code-block:: guess

    Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
        19                                           @profile
        20                                           def approxMagnitude(x, y):
        21         1         13.0     13.0     65.0      small = min(x, y)
        22         1          4.0      4.0     20.0      big = max(x, y)
        23         1          3.0      3.0     15.0      return 0.414 * small + big

*************
LLVM Numba
*************

LLVM provides a way to debug the vectorization performance of Numba. To enable this, add the following lines before the :code:`@numba.jit` decorator.::

    import llvmlite.binding as llvm

    llvm.set_option('','--debug-only=loop-vectorize')

LLVM output shows the assembly generated for the code. This gives some useful pointers to understand what's going on without having to learn assembly::

    ....
    LV: Vector loop of width 2 costs: 14.
    LV: Found an estimated cost of 0 for VF 4 For instruction:   %sum.221 = phi double [ 0.000000e+00, %B58.lr.ph ], [ %.607, %B58 ]
    LV: Found an estimated cost of 0 for VF 4 For instruction:   %.378.020 = phi i64 [ %.239.024, %B58.lr.ph ], [ %.436, %B58 ]
    LV: Found an estimated cost of 0 for VF 4 For instruction:   %.376.019 = phi i64 [ 0, %B58.lr.ph ], [ %.440, %B58 ]
    LV: Found an estimated cost of 1 for VF 4 For instruction:   %.436 = add nsw i64 %.378.020, -1
    LV: Found an estimated cost of 1 for VF 4 For instruction:   %.440 = add nuw nsw i64 %.376.019, 1
    LV: Found an estimated cost of 8 for VF 4 For instruction:   %.522 = mul i64 %.376.019, %arg.cholesky.6.1

Note the following from LLVM documentation,::

    The cost results are unit-less and the cost number represents the 
    throughput of the machine assuming that all loads hit the cache, 
    all branches are predicted, etc.

You may see, for example, statements like:: 

    LV: We can vectorize this loop!

or ::

    LV: Not vectorizing: loop did not meet vectorization requirements.

Having all loops vectorized may not lead to fastest code. For example, using fastmath=True forced LLVM to vectorize a loop, but led to slower code. 

Another decorator :code:`@vectorize` is useful for creating vectorized functions that perform pairwise operations on the elements of an array. An extended version :code:`@guvectorize` allows operation over a subset of elements of an array. Note that, :code:`@vectorize` or :code:`@guvectorize` cannot be mixed with :code:`@numba.jit` vectorization. It is best to try both and choose the one that suits your code the best and stick with it. 

:code:`@numba.jitclass` decorator allows entire classes to be JIT-compiled. A type specification of the members of the class needs to be provided; this allows the member data to be stored in C format, allowing member functions access to the raw data without having to go through the interpreter. This makes it more like :code:`C/C++`. Without a background in :code:`C/C++`, it can be hard to debug mistakes, especially as the interpreter support is limited in this mode, but it can be a boon for those who know :code:`C/C++` and want to bypass the interpreter. This is recommended in late stages of optimization when you have run out of all options and `still` want to stick with Python for the sake of readability.

There is also a :code:`@cfunc` decorator that allows you to create C-compatible callbacks for Python functions. 

****************************************************
Analyzing performance of Cholesky decomposition
****************************************************

We chose Cholesky decomposition as the algorithm to do the comparison on because it is used in a wide variety of fields, is well understood, there are highly optimized versions of the algorithms available in every language to benchmark performance. Reasonably efficient implementations of Cholesky decomposition have been around for a while now, but `efficient` is a moving target in the face of ever-changing hardware baseline. This remains an active area of research [#f2]_.

We will use the :code:`A = L L^T` variant of Cholesky decomposition because this variant is cache-friendly. Using :code:`A = LDL*` avoids computing square roots, but includes an extra multiplication in an :code:`O(N^3)` loop, whereas the square root occurs in a :code:`O(N)` loop.

We will review the following variants of the same algorithm

1. Python implementation
2. NumPy implementation (uses Numpy matrices but does not use np.linalg package)
3. Numba implementation
4. numpy.linalg.cholesky 
5. scipy.linalg.cholesky
6. scipy.linalg.lapack.cholesky
7. scikit-cuda

We use numpy and scipy implementations to set ourselves a target, rather than to evaluate them. The aim here is to see how far we can go from a pure Python implementation to Numba one without putting in a lot of effort or sacrificing readbility of the code [#f1]_ .  

.. image:: images/python-graphs.png
    :width: 800px
    :align: center
    :alt: alternate text

*****************************
Conclusion
*****************************

* If there is an optimized implementation available for your algorithm (such as scipy.linalg.lapack.cholesky), use that instead! 
* Numba can get you pretty close to optimized algorithms with very little effort (if the algorithm lends itself to vectorization). This is useful when you want to optimize *your* code, not a standard linear algebra routine. 

.. rubric:: Footnotes

.. [#f2] Cholesky Factorization on SIMD multi-core architectures `Paper <https://hal.archives-ouvertes.fr/hal-01550129/document>`_
.. [#f1] If we have to sacrifice readability to such an extent that a new user cannot immediately comprehend the code, then we might as well write the code in a lower level language such as C/C++ and maximise speed gains.


*****************************
References
*****************************

- `Numba documentation <https://numba.pydata.org/numba-doc/dev/user/jit.html>`_
- `Comparison of C, Julia and Cython for LU decomposition <https://www.ibm.com/developerworks/community/blogs/jfp/entry/A_Comparison_Of_C_Julia_Python_Numba_Cython_Scipy_and_BLAS_on_LU_Factorization?lang=en>`_
- `Numba under the hood <https://blog.zopa.com/2018/03/27/optimisation-algorithms-zopa-part-ii-speeding-python-numba/>`_
- `Mark Harrisâ€™ blog on Numba <https://devblogs.nvidia.com/numba-python-cuda-acceleration/>`_ 
- `Simple explanation of Numba <https://rushter.com/blog/numba-cython-python-optimization/>`_
- `Line profiler <https://github.com/rkern/line_profiler>`_